{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-ollama in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (0.2.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain-ollama) (0.3.37)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.3.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.10.3)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.27.0)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.6.2)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.7)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-ollama) (2.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain.prompts (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain.prompts\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.37 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain_community) (0.3.37)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain_community) (0.3.19)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain_community) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain_community) (3.11.12)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain_community) (9.0.0)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
      "  Using cached pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain_community) (0.3.8)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain_community) (2.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain<1.0.0,>=0.3.19->langchain_community) (0.3.6)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain<1.0.0,>=0.3.19->langchain_community) (2.10.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.2)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain_community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain_community) (2.27.1)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Using cached langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
      "Using cached marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
      "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain_community-0.3.18 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.7.1 python-dotenv-1.0.1 typing-inspect-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain_community.llms (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain_community.llms\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/bt4103/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "%pip install -U langchain-ollama\n",
    "%pip install langchain.prompts\n",
    "%pip install langchain_community\n",
    "%pip install langchain_community.llms\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.api.types import is_datetime64_any_dtype, is_numeric_dtype, is_object_dtype\n",
    "import os\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Try out to see how to run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- Return JSON's Headers and Description of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return JSON's Headers and Description of Data\n",
    "\n",
    "- Tells LLM what files are available and what headers are in each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Extract headers from all JSON files\u001b[39;00m\n\u001b[1;32m     55\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 56\u001b[0m json_schemas \u001b[38;5;241m=\u001b[39m recursive_json_schema_extractor(data_dir)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path, headers \u001b[38;5;129;01min\u001b[39;00m json_schemas\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[6], line 40\u001b[0m, in \u001b[0;36mrecursive_json_schema_extractor\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03mRecursively walks through the given directory and extracts headers\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mfrom all JSON files found.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    dict: A dictionary mapping JSON file paths to their header lists\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m schemas \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(directory):\n\u001b[1;32m     41\u001b[0m     full_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, entry)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(full_path):\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m# Recursively process subdirectories\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data'"
     ]
    }
   ],
   "source": [
    "def extract_headers(file_path):\n",
    "    \"\"\"\n",
    "    Extract headers from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        list: List of headers from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    headers = []\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        # For a dictionary, get the keys\n",
    "        headers = list(data.keys())\n",
    "    elif isinstance(data, list) and data:\n",
    "        first_item = data[0]\n",
    "        if isinstance(first_item, dict):\n",
    "            # For a list of dictionaries, get the keys from the first item\n",
    "            headers = list(first_item.keys())\n",
    "\n",
    "    return headers\n",
    "\n",
    "\n",
    "def recursive_json_schema_extractor(directory):\n",
    "    \"\"\"\n",
    "    Recursively walks through the given directory and extracts headers\n",
    "    from all JSON files found.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): The root directory to start the recursive search.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping JSON file paths to their header lists\n",
    "    \"\"\"\n",
    "    schemas = {}\n",
    "    for entry in os.listdir(directory):\n",
    "        full_path = os.path.join(directory, entry)\n",
    "        if os.path.isdir(full_path):\n",
    "            # Recursively process subdirectories\n",
    "            schemas.update(recursive_json_schema_extractor(full_path))\n",
    "        elif entry.lower().endswith(\".json\"):\n",
    "            try:\n",
    "                headers = extract_headers(full_path)\n",
    "                schemas[full_path] = headers\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {full_path}: {e}\")\n",
    "    return schemas\n",
    "\n",
    "\n",
    "# Extract headers from all JSON files\n",
    "data_dir = \"./data\"\n",
    "json_schemas = recursive_json_schema_extractor(data_dir)\n",
    "\n",
    "# Print the results\n",
    "for file_path, headers in json_schemas.items():\n",
    "    print(f\"\\nFile: {file_path}\")\n",
    "    print(\"Headers:\")\n",
    "    for header in headers:\n",
    "        print(f\"  {header}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Classifier\n",
    "\n",
    "- Decides which function to call based on the user's query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: Show me a graph of sales over time\n",
      "Type: chart\n",
      "\n",
      "Query: What is the average age of our customers?\n",
      "Type: description\n",
      "\n",
      "Query: Give me a complete analysis of our customer data\n",
      "Type: report\n"
     ]
    }
   ],
   "source": [
    "# Pydantic model for query classification\n",
    "class QueryType(BaseModel):\n",
    "    original_query: str\n",
    "    query_type: str  # Can be \"description\", \"report\", or \"chart\"\n",
    "\n",
    "\n",
    "# Template for query classification\n",
    "template = \"\"\"Classify the following query into one of these categories:\n",
    "- description: Queries asking for specific details or explanations about certain aspects\n",
    "- report: Queries requesting comprehensive analysis of all aspects of the dataset\n",
    "- chart: Queries specifically requesting visual representation or graphs\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Respond only in this exact format:\n",
    "original_query: [the original query]\n",
    "query_type: [description/report/chart]\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "\n",
    "# Function to parse LLM response into QueryType\n",
    "def parse_llm_response(response: str) -> QueryType:\n",
    "    lines = response.strip().split(\"\\n\")\n",
    "    parsed = {}\n",
    "    for line in lines:\n",
    "        key, value = line.split(\": \")\n",
    "        parsed[key] = value.strip()\n",
    "\n",
    "    return QueryType(**parsed)\n",
    "\n",
    "\n",
    "# Create the chain with structured output\n",
    "chain = prompt | model | parse_llm_response\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def classify_query(user_query: str) -> QueryType:\n",
    "    result = chain.invoke({\"query\": user_query})\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test examples\n",
    "test_queries = [\n",
    "    \"Show me a graph of sales over time\",\n",
    "    \"What is the average age of our customers?\",\n",
    "    \"Give me a complete analysis of our customer data\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    result = classify_query(query)\n",
    "    print(f\"\\nQuery: {result.original_query}\")\n",
    "    print(f\"Type: {result.query_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation Creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the file and columns to create the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic model for chart information\n",
    "class ChartInfo(BaseModel):\n",
    "    file_name: str\n",
    "    x_axis: str\n",
    "    y_axis: str\n",
    "\n",
    "\n",
    "# Format json_schemas into a readable string for the prompt (without type info)\n",
    "def format_schemas_for_prompt(schemas):\n",
    "    formatted_str = \"\"\n",
    "    for file_path, headers in schemas.items():\n",
    "        file_name = os.path.basename(file_path)  # Get just the filename without path\n",
    "        # Assume headers is now a list; join just the header names.\n",
    "        headers_str = \", \".join(headers)\n",
    "        formatted_str += f\"{file_name}: [{headers_str}]\\n\"\n",
    "    return formatted_str\n",
    "\n",
    "\n",
    "# Template that includes instructions for the LLM\n",
    "template = \"\"\"Given the following JSON file headers, determine the most appropriate file and columns to create the visualization.\n",
    "\n",
    "Available JSON files and their headers:\n",
    "{json_headers}\n",
    "\n",
    "Chart request: {query}\n",
    "\n",
    "Respond only with the following information in this exact format:\n",
    "file_name: [selected json file name]\n",
    "x_axis: [column name for x-axis]\n",
    "y_axis: [column name for y-axis]\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "\n",
    "# Function to parse LLM output into ChartInfo\n",
    "def parse_llm_response(response: str) -> ChartInfo:\n",
    "    lines = response.strip().split(\"\\n\")\n",
    "    parsed = {}\n",
    "    for line in lines:\n",
    "        key, value = line.split(\": \")\n",
    "        parsed[key] = value.strip()\n",
    "\n",
    "    return ChartInfo(**parsed)\n",
    "\n",
    "\n",
    "# Create the chain with structured output\n",
    "chain = prompt | model | parse_llm_response\n",
    "\n",
    "# Example usage with json_schemas:\n",
    "formatted_headers = format_schemas_for_prompt(json_schemas)\n",
    "result = chain.invoke(\n",
    "    {\"json_headers\": formatted_headers, \"query\": \"Show me the revenue trends over time\"}\n",
    ")\n",
    "\n",
    "print(\"Selected file:\", result.file_name)\n",
    "print(\"X-axis:\", result.x_axis)\n",
    "print(\"Y-axis:\", result.y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe for selected file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(f\"./Data/{result.file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Chart Generator\n",
    "\n",
    "1. Properly imports all necessary data type checking functions from pandas\n",
    "2. Handles different data type combinations:\n",
    "    - Time series data (both datetime and string-based time columns)\n",
    "    - Numeric vs numeric (scatter plots)\n",
    "    - Categorical vs numeric (box plots)\n",
    "    - Categorical vs categorical (heatmaps)\n",
    "3. Includes automatic handling of:\n",
    "    - Label rotation for better readability\n",
    "    - Layout adjustments to prevent cutoff\n",
    "    - Proper sorting for time series data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graph(df: pd.DataFrame, x_col: str, y_col: str) -> None:\n",
    "    \"\"\"\n",
    "    Generate an appropriate visualization based on the data types of input columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        x_col (str): Column name for x-axis\n",
    "        y_col (str): Column name for y-axis\n",
    "    \"\"\"\n",
    "    # Set figure size and style\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "    # Determine data types\n",
    "    x_is_datetime = is_datetime64_any_dtype(df[x_col])\n",
    "    x_is_numeric = is_numeric_dtype(df[x_col])\n",
    "    x_is_categorical = is_object_dtype(df[x_col])\n",
    "\n",
    "    y_is_datetime = is_datetime64_any_dtype(df[y_col])\n",
    "    y_is_numeric = is_numeric_dtype(df[y_col])\n",
    "    y_is_categorical = is_object_dtype(df[y_col])\n",
    "\n",
    "    # Time series plot\n",
    "    if x_is_datetime and y_is_numeric:\n",
    "        sns.lineplot(data=df, x=x_col, y=y_col)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "    # Scatter plot for numeric vs numeric\n",
    "    elif x_is_numeric and y_is_numeric:\n",
    "        sns.scatterplot(data=df, x=x_col, y=y_col)\n",
    "\n",
    "    # Box plot for categorical vs numeric\n",
    "    elif x_is_categorical and y_is_numeric:\n",
    "        sns.boxplot(data=df, x=x_col, y=y_col)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "    # Bar plot for categorical vs numeric (alternative to box plot)\n",
    "    elif y_is_categorical and x_is_numeric:\n",
    "        sns.barplot(data=df, x=x_col, y=y_col)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "    # Heatmap for categorical vs categorical\n",
    "    elif x_is_categorical and y_is_categorical:\n",
    "        # Create contingency table\n",
    "        contingency = pd.crosstab(df[x_col], df[y_col])\n",
    "        sns.heatmap(contingency, annot=True, fmt=\"d\", cmap=\"YlOrRd\")\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported combination of data types\")\n",
    "\n",
    "    # Add labels and adjust layout\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    plt.title(f\"{y_col} vs {x_col}\")\n",
    "    plt.tight_layout()  # Prevent label cutoff\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_graph(df, result.x_axis, result.y_axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic model for column selection\n",
    "class ColumnSelections(BaseModel):\n",
    "    selections: list[tuple[str, str]]  # List of (filename, column) pairs\n",
    "\n",
    "\n",
    "# Template for column selection\n",
    "template = \"\"\"Given the following JSON file headers and their data types, determine which columns would be relevant to answer the query.\n",
    "\n",
    "Available JSON files and their headers (with types):\n",
    "{json_headers}\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Respond only with a list of [filename, column] pairs, one per line, in this exact format:\n",
    "[file1.json, column1]\n",
    "[file1.json, column2]\n",
    "[file2.json, column3]\n",
    "...etc\n",
    "\n",
    "Each pair should be unique and relevant to the query.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "\n",
    "# Function to parse LLM output into ColumnSelections\n",
    "def parse_llm_response(response: str) -> ColumnSelections:\n",
    "    lines = response.strip().split(\"\\n\")\n",
    "    selections = []\n",
    "    seen_pairs = set()  # To ensure uniqueness\n",
    "\n",
    "    for line in lines:\n",
    "        # Remove brackets and split by comma\n",
    "        clean_line = line.strip(\"[]\").split(\",\")\n",
    "        if len(clean_line) == 2:\n",
    "            filename = clean_line[0].strip()\n",
    "            column = clean_line[1].strip()\n",
    "            pair = (filename, column)\n",
    "\n",
    "            # Only add if we haven't seen this combination before\n",
    "            if pair not in seen_pairs:\n",
    "                selections.append(pair)\n",
    "                seen_pairs.add(pair)\n",
    "\n",
    "    return ColumnSelections(selections=selections)\n",
    "\n",
    "\n",
    "# Create the chain with structured output\n",
    "chain = prompt | model | parse_llm_response\n",
    "\n",
    "# Example usage with json_schemas:\n",
    "formatted_headers = format_schemas_for_prompt(json_schemas)\n",
    "result = chain.invoke(\n",
    "    {\n",
    "        \"json_headers\": formatted_headers,\n",
    "        \"query\": \"What information do we have about customer purchases and their demographics?\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"\\nRelevant columns:\")\n",
    "for filename, column in result.selections:\n",
    "    print(f\"- {filename}: {column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\"\n",
    "    Custom JSON Encoder that converts numpy data types\n",
    "    into native Python types so they can be serialized.\n",
    "    \"\"\"\n",
    "\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super(NumpyEncoder, self).default(obj)\n",
    "\n",
    "\n",
    "def collect_statistics_report(result: ColumnSelections) -> list:\n",
    "    \"\"\"\n",
    "    Collects comprehensive statistical report data for selected columns,\n",
    "    returning a JSON-serializable list of statistics per (filename, column) pair.\n",
    "\n",
    "    Parameters:\n",
    "        result (ColumnSelections): The Pydantic model containing the list of (filename, column) pairs.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries with detailed statistics for each selected column.\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    for filename, column in result.selections:\n",
    "        try:\n",
    "            # Read the JSON file\n",
    "            df = pd.read_json(f\"./Data/{filename}\")\n",
    "\n",
    "            if pd.api.types.is_numeric_dtype(df[column]):\n",
    "                stats = {\n",
    "                    \"Count\": df[column].count(),\n",
    "                    \"Missing Values\": df[column].isnull().sum(),\n",
    "                    \"Mean\": df[column].mean(),\n",
    "                    \"Median\": df[column].median(),\n",
    "                    \"Mode\": (\n",
    "                        df[column].mode().iloc[0]\n",
    "                        if not df[column].mode().empty\n",
    "                        else None\n",
    "                    ),\n",
    "                    \"Std Dev\": df[column].std(),\n",
    "                    \"Min\": df[column].min(),\n",
    "                    \"Max\": df[column].max(),\n",
    "                    \"Q1 (25th percentile)\": df[column].quantile(0.25),\n",
    "                    \"Q3 (75th percentile)\": df[column].quantile(0.75),\n",
    "                    \"IQR\": df[column].quantile(0.75) - df[column].quantile(0.25),\n",
    "                }\n",
    "            else:\n",
    "                # For categorical columns, calculate unique count, missing values, and frequency counts.\n",
    "                value_counts = df[column].value_counts().to_dict()\n",
    "                stats = {\n",
    "                    \"Unique Values\": df[column].nunique(),\n",
    "                    \"Missing Values\": df[column].isnull().sum(),\n",
    "                    \"Value Frequencies\": value_counts,\n",
    "                }\n",
    "        except Exception as e:\n",
    "            stats = {\"error\": str(e)}\n",
    "\n",
    "        report.append({\"filename\": filename, \"column\": column, \"statistics\": stats})\n",
    "    return report\n",
    "\n",
    "\n",
    "# Example usage: Store the statistics in a JSON-serializable data structure.\n",
    "stats_report = collect_statistics_report(result)\n",
    "\n",
    "# Convert the report to a JSON string using the custom NumpyEncoder.\n",
    "stats_report_json = json.dumps(stats_report, indent=4, cls=NumpyEncoder)\n",
    "print(stats_report_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Based on the following JSON statistics report for various columns, generate a comprehensive written summary of the findings.\n",
    "\n",
    "JSON Statistics Report:\n",
    "{stats_report}\n",
    "\n",
    "Please provide a detailed summary including key insights, trends, and any anomalies in the data.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "chain = prompt | model\n",
    "final_report = chain.invoke({\"stats_report\": stats_report_json})\n",
    "\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report Generator\n",
    "\n",
    "- With the query this is what it does:\n",
    "    1. It will brainstorm what columns are relevant to the query\n",
    "    2. It will generate a list of queries that are relevant to the report requested\n",
    "    3. It will generate a report based on the queries and the data using the previous 2 functions\n",
    "    4. It will return the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DEBUG] Generating analysis queries:\n",
      "User query: Generate a report\n",
      "\n",
      "[DEBUG] Formatting schemas:\n",
      "- Processing file: Adjusted_Ad_Campaign_Performance_Data.json\n",
      "  Headers: Time, Campaign_ID, Age_group, Channel_Name, Spending, Number_of_Views, Number_of_Leads\n",
      "- Processing file: Banking_KPI_Data.json\n",
      "  Headers: Time, Number_of_New_Accounts, Total_Base_Mn\n",
      "\n",
      "Formatted headers:\n",
      "Adjusted_Ad_Campaign_Performance_Data.json: [Time, Campaign_ID, Age_group, Channel_Name, Spending, Number_of_Views, Number_of_Leads]\n",
      "Banking_KPI_Data.json: [Time, Number_of_New_Accounts, Total_Base_Mn]\n",
      "\n",
      "\n",
      "Invoking LLM chain...\n",
      "\n",
      "[DEBUG] Parsing LLM response:\n",
      "Raw response:\n",
      "Here are some sub-queries that would help analyze the data comprehensively:\n",
      "\n",
      "[chart, \"generate a chart showing correlation between Spending and Number_of_Views\"]\n",
      "[description, \"analyze the distribution of Age_group\"]\n",
      "[chart, \"generate a bar chart comparing spending across different Channel_Name categories\"]\n",
      "[chart, \"generate a scatter plot showing relationship between Total_Base_Mn and Number_of_New_Accounts\"]\n",
      "[description, \"describe the distribution of Time\"]\n",
      "\n",
      "Processing lines:\n",
      "  ⚠️ Error parsing line: Here are some sub-queries that would help analyze the data comprehensively:\n",
      "  ⚠️ Error parsing line: \n",
      "- Line: [chart, \"generate a chart showing correlation between Spending and Number_of_Views\"]\n",
      "  Type: chart\n",
      "  Text: generate a chart showing correlation between Spending and Number_of_Views\n",
      "  ✓ Added to queries\n",
      "- Line: [description, \"analyze the distribution of Age_group\"]\n",
      "  Type: description\n",
      "  Text: analyze the distribution of Age_group\n",
      "  ✓ Added to queries\n",
      "- Line: [chart, \"generate a bar chart comparing spending across different Channel_Name categories\"]\n",
      "  Type: chart\n",
      "  Text: generate a bar chart comparing spending across different Channel_Name categories\n",
      "  ✓ Added to queries\n",
      "- Line: [chart, \"generate a scatter plot showing relationship between Total_Base_Mn and Number_of_New_Accounts\"]\n",
      "  Type: chart\n",
      "  Text: generate a scatter plot showing relationship between Total_Base_Mn and Number_of_New_Accounts\n",
      "  ✓ Added to queries\n",
      "- Line: [description, \"describe the distribution of Time\"]\n",
      "  Type: description\n",
      "  Text: describe the distribution of Time\n",
      "  ✓ Added to queries\n",
      "\n",
      "Total queries parsed: 5\n",
      "\n",
      "Generated Analysis Queries:\n",
      "[chart] generate a chart showing correlation between Spending and Number_of_Views\n",
      "[description] analyze the distribution of Age_group\n",
      "[chart] generate a bar chart comparing spending across different Channel_Name categories\n",
      "[chart] generate a scatter plot showing relationship between Total_Base_Mn and Number_of_New_Accounts\n",
      "[description] describe the distribution of Time\n"
     ]
    }
   ],
   "source": [
    "# Pydantic model for query generation\n",
    "class QueryList(BaseModel):\n",
    "    queries: list[tuple[str, str]]\n",
    "\n",
    "\n",
    "# Format json_schemas into a readable string for the prompt (without type info)\n",
    "def format_schemas_for_prompt(schemas):\n",
    "    print(\"\\n[DEBUG] Formatting schemas:\")\n",
    "    formatted_str = \"\"\n",
    "    for file_path, headers in schemas.items():\n",
    "        file_name = os.path.basename(file_path)\n",
    "        headers_str = \", \".join(headers)\n",
    "        formatted_str += f\"{file_name}: [{headers_str}]\\n\"\n",
    "        print(f\"- Processing file: {file_name}\")\n",
    "        print(f\"  Headers: {headers_str}\")\n",
    "    return formatted_str\n",
    "\n",
    "\n",
    "# Template for generating sub-queries\n",
    "template = \"\"\"Given the following JSON file headers and a user query, generate a list of specific sub-queries that would help analyze the data comprehensively.\n",
    "\n",
    "Available JSON files and their headers:\n",
    "{json_headers}\n",
    "\n",
    "User Query: {query}\n",
    "\n",
    "Generate a list of sub-queries, each either requesting a chart or asking for descriptions.\n",
    "Each line MUST be in this exact format (use only these two types):\n",
    "[chart, \"generate a chart showing correlation between X and Y\"]\n",
    "[description, \"analyze the distribution of X\"]\n",
    "\n",
    "Rules:\n",
    "- Use ONLY 'chart' or 'description' as query types\n",
    "- For numeric columns, suggest charts comparing relevant pairs\n",
    "- For categorical columns, suggest descriptions of distributions\n",
    "- Keep queries focused and specific\n",
    "- Each query should be unique and relevant to the main question\n",
    "- Limit to 3-5 most relevant queries\n",
    "- Do not include any additional explanatory text\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = OllamaLLM(model=\"llama3.2\")\n",
    "\n",
    "\n",
    "# Function to parse LLM output into QueryList\n",
    "def parse_llm_response(response: str) -> QueryList:\n",
    "    print(\"\\n[DEBUG] Parsing LLM response:\")\n",
    "    print(f\"Raw response:\\n{response}\")\n",
    "\n",
    "    lines = response.strip().split(\"\\n\")\n",
    "    queries = []\n",
    "    seen_queries = set()\n",
    "\n",
    "    print(\"\\nProcessing lines:\")\n",
    "    for line in lines:\n",
    "        try:\n",
    "            clean_line = line.strip(\"[]\")\n",
    "            query_type, query_text = clean_line.split(\",\", 1)\n",
    "            query_type = query_type.strip().lower()\n",
    "            query_text = query_text.strip(' \"')\n",
    "\n",
    "            print(f\"- Line: {line}\")\n",
    "            print(f\"  Type: {query_type}\")\n",
    "            print(f\"  Text: {query_text}\")\n",
    "\n",
    "            if query_type not in [\"chart\", \"description\"]:\n",
    "                print(\"  ⚠️ Invalid query type - skipping\")\n",
    "                continue\n",
    "\n",
    "            query_pair = (query_type, query_text)\n",
    "            if query_pair not in seen_queries:\n",
    "                queries.append(query_pair)\n",
    "                seen_queries.add(query_pair)\n",
    "                print(\"  ✓ Added to queries\")\n",
    "            else:\n",
    "                print(\"  ⚠️ Duplicate query - skipping\")\n",
    "        except ValueError:\n",
    "            print(f\"  ⚠️ Error parsing line: {line}\")\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nTotal queries parsed: {len(queries)}\")\n",
    "    return QueryList(queries=queries)\n",
    "\n",
    "\n",
    "# Create the chain with structured output\n",
    "chain = prompt | model | parse_llm_response\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def generate_analysis_queries(user_query: str, json_schemas: dict) -> QueryList:\n",
    "    print(\"\\n[DEBUG] Generating analysis queries:\")\n",
    "    print(f\"User query: {user_query}\")\n",
    "\n",
    "    formatted_headers = format_schemas_for_prompt(json_schemas)\n",
    "    print(f\"\\nFormatted headers:\\n{formatted_headers}\")\n",
    "\n",
    "    print(\"\\nInvoking LLM chain...\")\n",
    "    result = chain.invoke({\"json_headers\": formatted_headers, \"query\": user_query})\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test example\n",
    "test_query = \"Generate a report\"\n",
    "result = generate_analysis_queries(test_query, json_schemas)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nGenerated Analysis Queries:\")\n",
    "for query_type, query in result.queries:\n",
    "    print(f\"[{query_type}] {query}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt4103",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
